{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ia8lTalyK2IV"
   },
   "outputs": [],
   "source": [
    "\"\"\"A simple world model\n",
    "\n",
    "Simple deterministic MDP is made of 6 grids (states)\n",
    "---------------------------------\n",
    "|         |          |          |\n",
    "|  Start  |          |  Goal    |\n",
    "|         |          |          |\n",
    "---------------------------------\n",
    "|         |          |          |\n",
    "|         |          |  Hole    |\n",
    "|         |          |          |\n",
    "---------------------------------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from termcolor import colored\n",
    "\n",
    "\n",
    "class QWorld:\n",
    "    def __init__(self):\n",
    "        \"\"\"Simulated deterministic world made of 6 states.\n",
    "        \"\"\"\n",
    "        # 4 actions\n",
    "        # 0 - Left, 1 - Down, 2 - Right, 3 - Up\n",
    "        self.col = 4\n",
    "\n",
    "        # 6 states\n",
    "        self.row = 6\n",
    "\n",
    "        # setup the environment\n",
    "        self.init_transition_table()\n",
    "        self.init_reward_table()\n",
    "\n",
    "        # reset the environment\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"start of episode\"\"\"\n",
    "        self.state = 0\n",
    "        self.count = 0\n",
    "        return self.state\n",
    "\n",
    "    def is_in_win_state(self):\n",
    "        \"\"\"agent wins when the goal is reached\"\"\"\n",
    "        return self.state == 2\n",
    "    \n",
    "#The reward table stores the resulting reward corresponding to pairs of (states, actions).\n",
    "\n",
    "    def init_reward_table(self):\n",
    "        \"\"\"\n",
    "        0 - Left, 1 - Down, 2 - Right, 3 - Up\n",
    "        ----------------\n",
    "        | 0 | 0 | 100  |\n",
    "        ----------------\n",
    "        | 0 | 0 | -100 |\n",
    "        ----------------\n",
    "        \"\"\"\n",
    "        #############################################\n",
    "        #TODO-- fill in the reward table\n",
    "        #############################################\n",
    "        self.reward_table = np.zeros([self.row, self.col])\n",
    "        # To be completed\n",
    "        #Action 0 : Left\n",
    "        self.reward_table[0, 0] = 0\n",
    "        self.reward_table[1, 0] = 0\n",
    "        self.reward_table[2, 0] = 0\n",
    "        self.reward_table[3, 0] = 0\n",
    "        self.reward_table[4, 0] = 0\n",
    "        self.reward_table[5, 0] = 0\n",
    "\n",
    "        # Action 1 : Down\n",
    "        self.reward_table[0, 1] = 0\n",
    "        self.reward_table[1, 1] = 0\n",
    "        self.reward_table[2, 1] = -100\n",
    "        self.reward_table[3, 1] = 0\n",
    "        self.reward_table[4, 1] = 0\n",
    "        self.reward_table[5, 1] = 0\n",
    "\n",
    "        # Action 2 : Right\n",
    "        self.reward_table[0, 2] = 0\n",
    "        self.reward_table[1, 2] = 100\n",
    "        self.reward_table[2, 2] = 0\n",
    "        self.reward_table[3, 2] = 0\n",
    "        self.reward_table[4, 2] = -100\n",
    "        self.reward_table[5, 2] = 0\n",
    "\n",
    "        # Action 3 : Up\n",
    "        self.reward_table[0, 3] = 0\n",
    "        self.reward_table[1, 3] = 0\n",
    "        self.reward_table[2, 3] = 0\n",
    "        self.reward_table[3, 3] = 0\n",
    "        self.reward_table[4, 3] = 0\n",
    "        self.reward_table[5, 3] = 100\n",
    "\n",
    "\n",
    "#The transition table stores the resulting states corresponding to pairs of (states, actions).\n",
    "    def init_transition_table(self):\n",
    "        \"\"\"\n",
    "        actions:\n",
    "        0 - Left, 1 - Down, 2 - Right, 3 - Up\n",
    "\n",
    "        states:\n",
    "        -------------\n",
    "        | 0 | 1 | 2 |\n",
    "        -------------\n",
    "        | 3 | 4 | 5 |\n",
    "        -------------\n",
    "        \"\"\"\n",
    "        self.transition_table = np.zeros([self.row, self.col],\n",
    "                                         dtype=int)\n",
    "\n",
    "        self.transition_table[0, 0] = 0\n",
    "        self.transition_table[0, 1] = 3\n",
    "        self.transition_table[0, 2] = 1\n",
    "        self.transition_table[0, 3] = 0\n",
    "\n",
    "        #############################################\n",
    "        #TODO-- complete the transition_table\n",
    "        #############################################\n",
    "        # to be completed\n",
    "        self.transition_table[1, 0] = 0\n",
    "        self.transition_table[1, 1] = 4\n",
    "        self.transition_table[1, 2] = 2\n",
    "        self.transition_table[1, 3] = 1\n",
    "\n",
    "        self.transition_table[2, 0] = 1\n",
    "        self.transition_table[2, 1] = 5\n",
    "        self.transition_table[2, 2] = 2\n",
    "        self.transition_table[2, 3] = 2\n",
    "\n",
    "        self.transition_table[3, 0] = 3\n",
    "        self.transition_table[3, 1] = 3\n",
    "        self.transition_table[3, 2] = 4\n",
    "        self.transition_table[3, 3] = 0\n",
    "\n",
    "        self.transition_table[4, 0] = 3\n",
    "        self.transition_table[4, 1] = 4\n",
    "        self.transition_table[4, 2] = 5\n",
    "        self.transition_table[4, 3] = 1\n",
    "\n",
    "        self.transition_table[5, 0] = 4\n",
    "        self.transition_table[5, 1] = 5\n",
    "        self.transition_table[5, 2] = 5\n",
    "        self.transition_table[5, 3] = 2\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"execute the action on the environment\n",
    "        Argument:\n",
    "            action (tensor): An action in Action space\n",
    "        Returns:\n",
    "            next_state (tensor): next env state\n",
    "            reward (float): reward received by the agent\n",
    "            done (Bool): whether the terminal state\n",
    "                is reached\n",
    "        \"\"\"\n",
    "        # determine the next_state given state and action\n",
    "        next_state = self.transition_table[self.state, action]\n",
    "        # done is True if next_state is Goal or Hole\n",
    "        #############################################\n",
    "        #TODO\n",
    "        #############################################\n",
    "        done =  self.is_in_win_state()\n",
    "\n",
    "        # reward given the state and action\n",
    "        reward = self.reward_table[self.state, action]\n",
    "        # the enviroment is now in new state\n",
    "        self.state = next_state\n",
    "        self.count+=1\n",
    "        return next_state, reward, done\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def print_cell(self, row=0):\n",
    "        \"\"\"UI to display agent moving on the grid\"\"\"\n",
    "        print(\"\")\n",
    "        for i in range(13):\n",
    "            j = i - 2\n",
    "            if j in [0, 4, 8]:\n",
    "                if j == 8:\n",
    "                    if self.state == 2 and row == 0:\n",
    "                        marker = \"\\033[4mG\\033[0m\"\n",
    "                    elif self.state == 5 and row == 1:\n",
    "                        marker = \"\\033[4mH\\033[0m\"\n",
    "                    else:\n",
    "                        marker = 'G' if row == 0 else 'H'\n",
    "                    color = self.state == 2 and row == 0\n",
    "                    color = color or (self.state == 5 and row == 1)\n",
    "                    color = 'red' if color else 'blue'\n",
    "                    print(colored(marker, color), end='')\n",
    "                elif self.state in [0, 1, 3, 4]:\n",
    "                    cell = [(0, 0, 0), (1, 0, 4), (3, 1, 0), (4, 1, 4)]\n",
    "                    marker = '_' if (self.state, row, j) in cell else ' '\n",
    "                    print(colored(marker, 'red'), end='')\n",
    "                else:\n",
    "                    print(' ', end='')\n",
    "            elif i % 4 == 0:\n",
    "                    print('|', end='')\n",
    "            else:\n",
    "                print(' ', end='')\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "    def print_world(self, action):\n",
    "        \"\"\"UI to display mode and action of agent\"\"\"\n",
    "        actions = { 0: \"(Left)\", 1: \"(Down)\", 2: \"(Right)\", 3: \"(Up)\" }\n",
    "        if self.count==0:\n",
    "          print(\"Start Game\")\n",
    "        else:\n",
    "          print(\"Action : \", actions[action])\n",
    "        for _ in range(13):\n",
    "            print('-', end='')\n",
    "        self.print_cell()\n",
    "        for _ in range(13):\n",
    "            print('-', end='')\n",
    "        self.print_cell(row=1)\n",
    "        for _ in range(13):\n",
    "            print('-', end='')\n",
    "        print(\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xM3FVC9cYlwl"
   },
   "outputs": [],
   "source": [
    "def print_episode(episode, delay=1):\n",
    "    \"\"\"UI to display episode count\n",
    "    Arguments:\n",
    "        episode (int): episode number\n",
    "        delay (int): sec delay\n",
    "\n",
    "    \"\"\"\n",
    "    os.system('clear')\n",
    "    for _ in range(13):\n",
    "        print('=', end='')\n",
    "    print(\"\")\n",
    "    print(\"Episode \", episode)\n",
    "    for _ in range(13):\n",
    "        print('=', end='')\n",
    "    print(\"\")\n",
    "    time.sleep(delay)\n",
    "\n",
    "def print_status(q_world, done,action, delay=1):\n",
    "    \"\"\"UI to display the world,\n",
    "        delay of 1 sec for ease of understanding\n",
    "    \"\"\"\n",
    "    os.system('clear')\n",
    "    q_world.print_world(action)\n",
    "    if done:\n",
    "        print(\"-------EPISODE DONE--------\")\n",
    "        delay *= 2\n",
    "    time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tCYOCOPBK7YR"
   },
   "outputs": [],
   "source": [
    "#instantiate the environment\n",
    "q_world = QWorld()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwvXZezFg2js"
   },
   "source": [
    "# **TODO:**\n",
    "\n",
    "\n",
    "A) Complete the code in the cells above (the parts to be completed are marked with 'TODO')\n",
    "\n",
    "B) Once part A) is complete, implement now each of the following situations in **a separate cell**. For the sake of illustration, Situation 1 is already implemented for you to give you a hint on how to answer each part. You could implement somthing similar for Situation 2, Situation 3 and Situation 4.\n",
    "\n",
    "**Situation 1:**take agent to goal in 2 steps. Print the episode name and display the grid for each step taken.\n",
    "\n",
    "**Situation 2:** take agent to H in 3 steps. Print the episode name and display the grid for each step taken.\n",
    "\n",
    "**Situation 3:** implement the following trajectory: down-right-up-right. what's the cumulative reward (assume the discount factor is 1)? Compare with the cumulative reward of the episode from **Situation 1** and comment.\n",
    "Make sure the cumulative reward is printed when the cell is executed.\n",
    "\n",
    "**Situation 4:** Implement an agent that takes random actions at each step and stops only when the task is solved (note that your agent may need to go through multiple episodes before your it is able to reach the goal). After how many episodes it solved the task? (the number of episodes should be displayed automatically each time you run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oUjj7jPFVSlU",
    "outputId": "be36a550-bd96-458f-e5f1-813e4b655916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Episode  1\n",
      "=============\n",
      "Start Game\n",
      "-------------\n",
      "| \u001b[31m_\u001b[0m | \u001b[31m \u001b[0m | \u001b[34mG\u001b[0m |\n",
      "-------------\n",
      "| \u001b[31m \u001b[0m | \u001b[31m \u001b[0m | \u001b[34mH\u001b[0m |\n",
      "-------------\n",
      "Action :  (Right)\n",
      "-------------\n",
      "| \u001b[31m \u001b[0m | \u001b[31m_\u001b[0m | \u001b[34mG\u001b[0m |\n",
      "-------------\n",
      "| \u001b[31m \u001b[0m | \u001b[31m \u001b[0m | \u001b[34mH\u001b[0m |\n",
      "-------------\n",
      "Action :  (Right)\n",
      "-------------\n",
      "|   |   | \u001b[31m\u001b[4mG\u001b[0m\u001b[0m |\n",
      "-------------\n",
      "|   |   | \u001b[34mH\u001b[0m |\n",
      "-------------\n",
      " The cumulative Reward for Situation 1 is equal to: 100.0\n"
     ]
    }
   ],
   "source": [
    "# Situation 1\n",
    "# #initialize the env\n",
    "state = q_world.reset()\n",
    "done = False\n",
    "episode = 1\n",
    "delay = 0\n",
    "print_episode(episode=episode, delay=0)\n",
    "\n",
    "# print initial status of the board\n",
    "print_status(q_world, done, 0, delay=delay)\n",
    "\n",
    "\n",
    "# to take the agent to GOAL (G) in two steps, the agent needs\n",
    "# to go right then go right again\n",
    "\n",
    "# recall the actions:\n",
    "# 0 - Left, 1 - Down, 2 - Right, 3 - Up\n",
    "\n",
    "# 1- Go right\n",
    "action=2\n",
    "next_state, reward, done = q_world.step(action)\n",
    "print_status(q_world, done,action, delay=delay)\n",
    "\n",
    "# 2- Go right again\n",
    "action=2\n",
    "next_state, reward, done = q_world.step(action)\n",
    "print_status(q_world, done,action, delay=delay)\n",
    "cumulative_reward_situation1 = reward\n",
    "print(f' The cumulative Reward for Situation 1 is equal to: {cumulative_reward_situation1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4JY-_Hm9MD6O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Episode  2\n",
      "=============\n",
      "Start Game\n",
      "-------------\n",
      "| \u001b[31m_\u001b[0m | \u001b[31m \u001b[0m | \u001b[34mG\u001b[0m |\n",
      "-------------\n",
      "| \u001b[31m \u001b[0m | \u001b[31m \u001b[0m | \u001b[34mH\u001b[0m |\n",
      "-------------\n",
      "Action :  (Down)\n",
      "-------------\n",
      "| \u001b[31m \u001b[0m | \u001b[31m \u001b[0m | \u001b[34mG\u001b[0m |\n",
      "-------------\n",
      "| \u001b[31m_\u001b[0m | \u001b[31m \u001b[0m | \u001b[34mH\u001b[0m |\n",
      "-------------\n",
      "Action :  (Right)\n",
      "-------------\n",
      "| \u001b[31m \u001b[0m | \u001b[31m \u001b[0m | \u001b[34mG\u001b[0m |\n",
      "-------------\n",
      "| \u001b[31m \u001b[0m | \u001b[31m_\u001b[0m | \u001b[34mH\u001b[0m |\n",
      "-------------\n",
      "Action :  (Right)\n",
      "-------------\n",
      "|   |   | \u001b[34mG\u001b[0m |\n",
      "-------------\n",
      "|   |   | \u001b[31m\u001b[4mH\u001b[0m\u001b[0m |\n",
      "-------------\n",
      " The cumulative Reward for Situation 2 is equal to: -100.0\n"
     ]
    }
   ],
   "source": [
    "# Implement Situation 2\n",
    "# TODO\n",
    "# #initialize the env\n",
    "state = q_world.reset()\n",
    "done = False\n",
    "episode = 2  # You can choose a different episode number\n",
    "delay = 0\n",
    "print_episode(episode=episode, delay=0)\n",
    "\n",
    "# print initial status of the board\n",
    "print_status(q_world, done, 0, delay=delay)\n",
    "\n",
    "# To take the agent to H in three steps, the agent needs\n",
    "# to go down, right, then right\n",
    "\n",
    "# 1- Go down\n",
    "action = 1\n",
    "next_state, reward, done = q_world.step(action)\n",
    "print_status(q_world, done, action, delay=delay)\n",
    "\n",
    "# 2- Go right\n",
    "action = 2\n",
    "next_state, reward, done = q_world.step(action)\n",
    "print_status(q_world, done, action, delay=delay)\n",
    "\n",
    "# 3- Go right\n",
    "action = 2\n",
    "next_state, reward, done = q_world.step(action)\n",
    "print_status(q_world, done, action, delay=delay)\n",
    "cumulative_reward_situation2 = reward\n",
    "print(f' The cumulative Reward for Situation 2 is equal to: {cumulative_reward_situation2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "D7pnyph8iITn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============\n",
      "Episode  0\n",
      "=============\n",
      "Start Game\n",
      "-------------\n",
      "| \u001b[31m_\u001b[0m | \u001b[31m \u001b[0m | \u001b[34mG\u001b[0m |\n",
      "-------------\n",
      "| \u001b[31m \u001b[0m | \u001b[31m \u001b[0m | \u001b[34mH\u001b[0m |\n",
      "-------------\n",
      "Action :  (Down)\n",
      "-------------\n",
      "| \u001b[31m \u001b[0m | \u001b[31m \u001b[0m | \u001b[34mG\u001b[0m |\n",
      "-------------\n",
      "| \u001b[31m_\u001b[0m | \u001b[31m \u001b[0m | \u001b[34mH\u001b[0m |\n",
      "-------------\n",
      "Action :  (Right)\n",
      "-------------\n",
      "| \u001b[31m \u001b[0m | \u001b[31m \u001b[0m | \u001b[34mG\u001b[0m |\n",
      "-------------\n",
      "| \u001b[31m \u001b[0m | \u001b[31m_\u001b[0m | \u001b[34mH\u001b[0m |\n",
      "-------------\n",
      "Action :  (Up)\n",
      "-------------\n",
      "| \u001b[31m \u001b[0m | \u001b[31m_\u001b[0m | \u001b[34mG\u001b[0m |\n",
      "-------------\n",
      "| \u001b[31m \u001b[0m | \u001b[31m \u001b[0m | \u001b[34mH\u001b[0m |\n",
      "-------------\n",
      "Action :  (Right)\n",
      "-------------\n",
      "|   |   | \u001b[31m\u001b[4mG\u001b[0m\u001b[0m |\n",
      "-------------\n",
      "|   |   | \u001b[34mH\u001b[0m |\n",
      "-------------\n",
      " The cumulative Reward for Situation 3 is equal to: 100.0\n"
     ]
    }
   ],
   "source": [
    "# Implement Situation 3\n",
    "# TODO\n",
    "# Situation 3\n",
    "#initialize the env\n",
    "state = q_world.reset()\n",
    "done = False\n",
    "episode = 0 \n",
    "delay = 0\n",
    "print_episode(episode=episode, delay=0)\n",
    "\n",
    "print_status(q_world, done, 0, delay=delay)\n",
    "\n",
    "# Trajectory: down-right-up-right\n",
    "# 1- Go down\n",
    "action = 1\n",
    "next_state, reward, done = q_world.step(action)\n",
    "print_status(q_world, done, action, delay=delay)\n",
    "\n",
    "# 2- Go right\n",
    "action = 2\n",
    "next_state, reward, done = q_world.step(action)\n",
    "print_status(q_world, done, action, delay=delay)\n",
    "\n",
    "# 3- Go up\n",
    "action = 3\n",
    "next_state, reward, done = q_world.step(action)\n",
    "print_status(q_world, done, action, delay=delay)\n",
    "\n",
    "# 4- Go right\n",
    "action = 2\n",
    "next_state, reward, done = q_world.step(action)\n",
    "print_status(q_world, done, action, delay=delay)\n",
    "\n",
    "# Print cumulative reward\n",
    "cumulative_reward_situation3 = reward\n",
    "print(f' The cumulative Reward for Situation 3 is equal to: {cumulative_reward_situation3}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cumulative Reward for Situation 1 and 3 are both equal to: 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "jdApmbmYXWdS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Situation 4 took 11 episodes to complete.\n"
     ]
    }
   ],
   "source": [
    "# Implement Situation 4\n",
    "# TODO\n",
    "# Situation 4\n",
    "import random\n",
    "\n",
    "# checking if the task is solved\n",
    "def is_challenge_completed():\n",
    "    return q_world.is_in_win_state()\n",
    "\n",
    "# initialize the env\n",
    "state = q_world.reset()\n",
    "done = False\n",
    "episode = 0\n",
    "delay = 0\n",
    "\n",
    "total_episodes = 0\n",
    "\n",
    "# Continue taking random actions until the task is solved\n",
    "while not is_challenge_completed():\n",
    "    # Choose a random action (0: Left, 1: Down, 2: Right, 3: Up)\n",
    "    random_action = random.randint(0, 3)\n",
    "    \n",
    "    # Execute the action in the environment\n",
    "    next_state, reward, done = q_world.step(random_action)\n",
    "\n",
    "    # Increment episode count\n",
    "    total_episodes += 1\n",
    "    \n",
    "# Print the number of episodes it took to solve the challenge\n",
    "print(f'Situation 4 took {total_episodes} episodes to complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
